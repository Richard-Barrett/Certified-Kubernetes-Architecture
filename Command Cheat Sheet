1. View the pods in the default namespace with a custom view:
kubectl get pods -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system

2. View the kube-scheduler YAML:
kubectl get endpoints kube-scheduler -n kube-system -o yaml

3.Create a stacked etcd topology using kubeadm:
kubeadm init --config=kubeadm-config.yaml

4. Watch as pods are created in the default namespace:
kubectl get pods -n kube-system -w

#Configuring Secure Cluster Communications
To prevent unauthorized users from modifying the cluster state, RBAC is used, defining roles and role bindings for a user. 
A service account resource is created for a pod to determine how it has control over the cluster state. 
For example, the default service account will not allow you to list the services in a namespace.

1. View the kube-config:
cat .kube/config | more

2. View the service account token:
kubectl get secrets

3. Create a new namespace named my-ns:
kubectl create ns my-ns

4. Run the kube-proxy pod in the my-ns namespace:
kubectl run test --image=chadmcrowell/kubectl-proxy -n my-ns

5. List the pods in the my-ns namespace:
kubectl get pods -n my-ns

6. Run a shell in the newly created pod:
kubectl exec -it <name-of-pod> -n my-ns sh

7. List the services in the namespace via API call:
curl localhost:8001/api/v1/namespaces/my-ns/services

8. View the token file from within a pod:
cat /var/run/secrets/kubernetes.io/serviceaccount/token

9. List the service account resources in your cluster:
kubectl get serviceaccounts

Helpful Links
https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
http://kubernetes.io/docs/admin
https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/#securing-a-cluster
https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
https://kubernetes.io/docs/reference/access-authn-authz/authorization/
https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/

# Running End-to-End Tests on Your Cluster
Running end-to-end tests ensures your application will run efficiently without having to worry about cluster health problems. 
Kubetest is a useful tool for providing end-to-end tests â€” however, it is beyond the scope of this exam. 
In this lesson, we will go through the practice of testing our ability to run deployments, run pods, expose a container, execute a command from a container, run a service, and check the overall health of our nodes and pods for conditions.

1. Run a simple nginx deployment:
kubectl run nginx --image=nginx

2. View the deployments in your cluster:
kubectl get deployments

3. View the pods in the cluster:
kubectl get pods

4. Use port forwarding to access a pod directly:
kubectl port-forward $pod_name 8081:80

5. Get a response from the nginx pod directly:
curl --head http://127.0.0.1:8081

6. View the logs from a pod:
kubectl logs $pod_name

7. Run a command directly from the container:
kubectl exec -it nginx -- nginx -v

8. Create a service by exposing port 80 of the nginx deployment:
kubectl expose deployment nginx --port 80 --type NodePort

9. List the services in your cluster:
kubectl get services

10. Get a response from the service:
curl -I localhost:$node_port

11. List the nodes' status:
kubectl get nodes

12. View detailed information about the nodes:
kubectl describe nodes

13. View detailed information about the pods:
kubectl describe pods

Helpful Links:
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-testing/e2e-tests.md
https://kubernetes.io/docs/getting-started-guides/ubuntu/

#Upgrading the Kubernetes Cluster
kubeadm allows us to upgrade our cluster components in the proper order, making sure to include important feature upgrades we might want to take advantage of in the latest stable version of Kubernertes. 
In this lesson, we will go through upgrading our cluster from version 1.13.5 to 1.14.1.

1. View the version of the server and client on the master node:
kubectl version --short

2. View the version of the scheduler and controller manager:
kubectl get pods -n kube-system kube-controller-manager-chadcrowell1c.mylabserver.com -o yaml

3. View the name of the kube-controller pod:
kubectl get pods -n kube-system

4. Set the VERSION variable to the latest stable release of Kubernetes:
export VERSION=v1.14.1

5. Set the ARCH variable to the amd64 system:
export ARCH=amd64

6. View the latest stable version of Kubernetes using the variable:
echo $VERSION

7. Curl the latest stable version of Kubernetes:
curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubeadm > kubeadm

8. Install the latest version of kubeadm:
sudo install -o root -g root -m 0755 ./kubeadm /usr/bin/kubeadm

9. Check the version of kubeadm:
sudo kubeadm version

10. Plan the upgrade:
sudo kubeadm upgrade plan

11. Apply the upgrade to 1.14.1:
kubeadm upgrade apply v1.14.1

12. View the differences between the old and new manifests:
diff kube-controller-manager.yaml /etc/kubernetes/manifests/kube-controller-manager.yaml

13. Curl the latest version of kubelet:
curl -sSL https://dl.k8s.io/release/${VERSION}/bin/linux/${ARCH}/kubelet > kubelet

14. Install the latest version of kubelet:
sudo install -o root -g root -m 0755 ./kubelet /usr/bin/kubelet

15. Restart the kubelet service:
sudo systemctl restart kubelet.service

16. Watch the nodes as they change version:
kubectl get nodes -w

Helpful Links
=============
https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.13.md

# Operating System Upgrades within a Kubernetes Cluster
When we need to take a node down for maintenance, Kubernetes makes it easy to evict the pods on that node, take it down, and then continue scheduling pods after the maintenance is complete. 
Furthermore, if the node needs to be decommissioned, you can just as easily remove the node and replace it with a new one, joining it to the cluster.

1. See which pods are running on which nodes:
kubectl get pods -o wide

2. Evict the pods on a node:
kubectl drain [node_name] --ignore-daemonsets

3. Watch as the node changes status:
kubectl get nodes -w

4. Schedule pods to the node after maintenance is complete:
kubectl uncordon [node_name]

5. Remove a node from the cluster:
kubectl delete node [node_name]

6. Generate a new token:
sudo kubeadm token generate

7. List the tokens:
sudo kubeadm token list

8. Print the kubeadm join command to join a node to the cluster:
sudo kubeadm token create [token_name] --ttl 2h --print-join-command
